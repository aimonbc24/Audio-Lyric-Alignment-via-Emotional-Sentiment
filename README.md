# Audio-to-Language-Alignment-via-Musical-Sentiment


In this research, we investigate the feasibility of using musical sentiment as a method for aligning audio and lyrics in songs, an area that has remained largely unexplored in Music Information Retrieval (MIR) and Music Emotion Recognition (MER). The study utilizes the DALI dataset, comprising over 5000 songs with synchronized audio, lyrics, and notes, to develop a multi-modal text-audio model. This model aims to bridge the gap between the sentiment in song lyrics and its corresponding audio signature. The approach involves segmenting songs into musical sections, generating MEL spectrograms for audio analysis, and applying sentiment analysis to the lyrics, followed by training a Vision Transformer (ViT) using contrastive learning to create a joint embedding space for audio spectrograms and text-based sentiment descriptions. This method diverges from traditional fixed-vocabulary sentiment analysis, allowing for more nuanced and diverse emotion recognition. The potential applications of the resulting model are broad, including sentiment-based song search and recommendation, and enhancement of music composition by aligning lyrical sentiment with audio.
